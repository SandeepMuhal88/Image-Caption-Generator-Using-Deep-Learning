{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c4d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fff430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "# %pip install tqdm\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6284e1aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras' from 'tensorflow' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VGG16, preprocess_input\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_img, img_to_array\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'keras' from 'tensorflow' (unknown location)"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d900ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, add\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical,plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb77cd",
   "metadata": {},
   "source": [
    "Make Model using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d134e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Model\n",
    "model = VGG16()\n",
    "#Restructure the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "#summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb668ad",
   "metadata": {},
   "source": [
    "Feature Extraction Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08637ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "# If you want to go inside \"Images\" or \"captions.txt\"\n",
    "images_dir = os.path.join(path, \"Images\")\n",
    "captions_file = os.path.join(path, \"captions.txt\") #that is use in caption generation\n",
    "\n",
    "for img in os.listdir(images_dir):\n",
    "    # Only process image files\n",
    "    if not img.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "        continue\n",
    "    img_path = images_dir + '/' + img\n",
    "    #resize image\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    #convert image pixels to array\n",
    "    image = img_to_array(image)\n",
    "    #reshape data for modele  \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    #prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    #get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    #get image id\n",
    "    image_id = img.split('.')[0]\n",
    "    #store features in dictionary\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad886fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e883975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base dir:\", os.getcwd())  \n",
    "# File ka asli location (root ke hisaab se)\n",
    "\n",
    "print(\"Working dir:\", os.getcwd())  \n",
    "# Jaha se tumne program run kiya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f849006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save features to file\n",
    "pickle.dump(features,open(os.path.join(os.getcwd(), 'features.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load features from pickle\n",
    "with open(os.path.join(os.getcwd(), 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d735a",
   "metadata": {},
   "source": [
    "Load Caption Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be999998",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(captions_file, 'r') as f:\n",
    "    next(f)  # Skip the header line\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d236f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(captions_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "str=\"1003163366_44323f5815.jpg,a man sleeping on a bench outside with a white and black dog sitting next to him .\"\n",
    "\n",
    "tokens = str.split(',')\n",
    "print(tokens)\n",
    "image_id = tokens[0].split('.')[0]\n",
    "print(image_id) #image id without extension\n",
    "caption = tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for line in (captions_doc.split('\\n')):\n",
    "    tokens = line.split(',')\n",
    "    if len(line)<2:\n",
    "        continue\n",
    "    #remove extension from image id\n",
    "    image_id, caption = tokens[0].split('.')[0], tokens[1:]\n",
    "    #convert caption list to string\n",
    "    caption = ' '.join(caption)\n",
    "    #create the list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping = {}\n",
    "# for line in tqdm(captions_doc.split('\\n')):\n",
    "#     #split the line by comma(,)\n",
    "#     tokens = line.split(',')\n",
    "#     if len(line) < 2:\n",
    "#         continue\n",
    "#     image_id, caption = tokens[0], tokens[1:]\n",
    "#     image_id = image_id.split('.')[0]\n",
    "#     caption = ' '.join(caption)\n",
    "#     if image_id not in mapping:\n",
    "#         mapping[image_id] = []\n",
    "#     mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53aa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            #convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            #remove punctuation\n",
    "            caption = caption.replace('[^a-zA-Z]', ' ')\n",
    "            #remove multiple spaces\n",
    "            caption = ' '.join(caption.split())\n",
    "            #add start and end tokens to the caption\n",
    "            caption = 'startseq ' + caption + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4997120",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee429cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27295d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3703d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the maximum length of the captions\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc8b157",
   "metadata": {},
   "source": [
    "### Train Test plit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split\n",
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids)*0.80)\n",
    "train_image_ids = image_ids[:split]\n",
    "test_image_ids = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generator data in batches avoids Session time out\n",
    "\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def DataGenerator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over the data\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X,y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(features[key][0])   # image features\n",
    "                    X2.append(in_seq)             # input sequence\n",
    "                    y.append(out_seq)             # output word\n",
    "            if n == batch_size:\n",
    "                # convert to numpy arrays before yielding\n",
    "                X1 = np.array(X1, dtype=np.float32)\n",
    "                X2 = np.array(X2, dtype=np.float32)\n",
    "                y = np.array(y, dtype=np.float32)\n",
    "                yield [X1, X2], y\n",
    "                # reset\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aab988",
   "metadata": {},
   "source": [
    "Model Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c392738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Model\n",
    "#image feature extractor model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1=Dropout(0.4)(inputs1)\n",
    "fe2=Dense(256,activation='relu')(fe1)\n",
    "\n",
    "#sequence model\n",
    "inputs2=Input(shape=(max_length,))\n",
    "se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2=Dropout(0.4)(se1)\n",
    "se3=LSTM(256)(se2)\n",
    "\n",
    "#decoder model\n",
    "decoder1=add([fe2,se3])\n",
    "decoder2=Dense(256,activation='relu')(decoder1)\n",
    "outputs=Dense(vocab_size,activation='softmax')(decoder2)\n",
    "model=Model(inputs=[inputs1,inputs2],outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "\n",
    "# plot_model(model,show_shapes=True)\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "epochs=15\n",
    "batch_size=64\n",
    "\n",
    "steps=len(train_image_ids)//batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    generator = lambda: DataGenerator(\n",
    "        train_image_ids, mapping, features, tokenizer, max_length, vocab_size, batch_size\n",
    "    )\n",
    "\n",
    "    dataset = tensorflow.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            (tensorflow.TensorSpec(shape=(None, 4096), dtype=tensorflow.float32),     # X1: VGG16 features\n",
    "            tensorflow.TensorSpec(shape=(None, max_length), dtype=tensorflow.int32)), # X2: sequence tokens\n",
    "            tensorflow.TensorSpec(shape=(None, vocab_size), dtype=tensorflow.float32)  # y: one-hot labels\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.fit(dataset, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
